# Website Crawler

This project is a simple website crawler that extracts all the links from a given website. It fetches the content of the website, parses it to find all the links, and saves them to a file.

## Table of Contents

- [Files](#files)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Dependencies](#dependencies)
- [Functions](#functions)
- [Contributing](#contributing)
- [License](#license)

## Files

- `main.py`: The main script that performs the crawling.
- `requirements.txt`: The list of dependencies required for the project.

## Installation

1. Clone the repository:

   ```sh
   git clone https://github.com/MRX2424/website_link_extractor.git
   cd website_link_extractor
